{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FindProff.**"
      ],
      "metadata": {
        "id": "JZM4g2WEM8NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script begins by accepting keywords and a university URL, such as the faculty directory URL of Stevensons Institue of Technology or the University of Syracuse, as inputs. These keywords might include terms like \"water quality,\" \"machine learning,\" \"remote sensing,\" and \"hydrology.\" These will based on your research interests.\n",
        "\n",
        "Next, it retrieves all URLs linked from the primary URL and converts them into a structured soup object using BeautifulSoup. It then searches this object for occurrences of the specified keywords. If any of these keywords are found, the script extracts and returns the URL of the page where the keyword appears, along with the professor's name and email address associated with that page."
      ],
      "metadata": {
        "id": "EEfCnDqVM8ua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvziSL99ztlv",
        "outputId": "b9db40da-0f06-4a71-e3fd-e37b0ae755be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Function to extract email addresses from text using regex\n",
        "def extract_emails(text):\n",
        "    emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "    return emails\n",
        "\n",
        "# Function to extract valid absolute URLs from a primary URL\n",
        "def extract_urls(primary_url):\n",
        "    try:\n",
        "        # Send a GET request to the primary URL\n",
        "        response = requests.get(primary_url)\n",
        "\n",
        "        # Check if request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find all 'a' tags with 'href' attribute\n",
        "            links = soup.find_all('a', href=True)\n",
        "\n",
        "            # Extract and filter URLs from href attributes\n",
        "            urls = []\n",
        "            for link in links:\n",
        "                url = link.get('href')\n",
        "                if url:\n",
        "                    # If it's a relative URL, convert it to absolute URL\n",
        "                    absolute_url = urljoin(primary_url, url)\n",
        "                    if absolute_url.startswith('http://') or absolute_url.startswith('https://'):\n",
        "                        urls.append(absolute_url)\n",
        "                    else:\n",
        "                        # If it's neither http:// nor https://, skip it\n",
        "                        continue\n",
        "\n",
        "            # # Filter out non-absolute URLs and keep only those starting with 'https://www.stonybrook.edu/commcms/civileng/people/_core_faculty/'\n",
        "            # urls = [url for url in urls if url.startswith('https://www.stonybrook.edu/commcms/civileng/people/_core_faculty/')]\n",
        "\n",
        "            return urls\n",
        "        else:\n",
        "            # Request was not successful, handle error\n",
        "            print(f\"Error fetching {primary_url}: Status Code {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {primary_url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {primary_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Primary URL to extract URLs from\n",
        "primary_url = 'https://www.stevens.edu/school-engineering-science/departments/civil-environmental-ocean-engineering/faculty'\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = ['water quality', 'machine learning', 'remote sensing', 'hydrology']\n",
        "\n",
        "# Function to scrape URLs, search for keywords, and extract emails\n",
        "def scrape_urls(urls, keywords):\n",
        "    visited_urls = set()  # Set to keep track of visited URLs\n",
        "    for url in urls:\n",
        "        if url not in visited_urls:\n",
        "            visited_urls.add(url)  # Mark URL as visited\n",
        "\n",
        "            try:\n",
        "                # Fetch the webpage content\n",
        "                response = requests.get(url)\n",
        "                if response.status_code == 200:\n",
        "                    html = response.text\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                    # Extract title of the page\n",
        "                    title = soup.title.text.strip() if soup.title else \"No title\"\n",
        "\n",
        "                    # Check for keywords in the page content\n",
        "                    found_keywords = []\n",
        "                    for keyword in keywords:\n",
        "                        if soup.body and soup.body.find_all(string=re.compile(r'\\b{}\\b'.format(re.escape(keyword))), recursive=True):\n",
        "                            found_keywords.append(keyword)\n",
        "\n",
        "                    # Extract email addresses from the page content\n",
        "                    emails_in_page = extract_emails(html)\n",
        "\n",
        "                    # If any keywords are found, print the results\n",
        "                    if found_keywords:\n",
        "                        print(f\"URL: {url}\")\n",
        "                        print(f\"Title: {title}\")\n",
        "                        print(f\"Keywords found: {', '.join(found_keywords)}\")\n",
        "                        print(f\"Emails found: {', '.join(set(emails_in_page))}\")  # Use set() to remove duplicates\n",
        "                        print(\"---------------------------------------------\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching URL: {url}\")\n",
        "                print(e)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL: {url}\")\n",
        "                print(e)\n",
        "\n",
        "# Extract URLs from primary URL\n",
        "urls = extract_urls(primary_url)\n",
        "\n",
        "# Call the function to scrape URLs, search for keywords, and extract emails\n",
        "scrape_urls(urls, keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSMBn9ZU9x1q",
        "outputId": "c89a34e9-bd63-4886-b11c-594f3661dd01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.stevens.edu/academics/academics-at-stevens\n",
            "Title: Academics - Stevens Institute of Technology  | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: \n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/academics/undergraduate-study/success-the-stevens-core-curriculum\n",
            "Title: SUCCESS â€“ The Stevens Core Curriculum | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: \n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/academics/graduate-study\n",
            "Title: Graduate Study | Stevens Institute of Technology | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: \n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/ybao3\n",
            "Title: Yi Bao | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: ybao3@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/cchen6\n",
            "Title: Cheng Chen | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: cchen6@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/ddonskoy\n",
            "Title: Dimitri Donskoy | Stevens Institute of Technology\n",
            "Keywords found: machine learning, remote sensing\n",
            "Emails found: ddonskoy@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/sjagupil\n",
            "Title: Sarath Chandra Kumar Jagupilla | Stevens Institute of Technology\n",
            "Keywords found: water quality, machine learning, hydrology\n",
            "Emails found: sjagupil@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/kliu24\n",
            "Title: Kaijian Liu | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: kliu24@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/wmeng3\n",
            "Title: Weina Meng | Stevens Institute of Technology\n",
            "Keywords found: machine learning\n",
            "Emails found: wmeng3@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/dsarkar\n",
            "Title: Dibs Sarkar | Stevens Institute of Technology\n",
            "Keywords found: water quality\n",
            "Emails found: dsarkar@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/mtemimi\n",
            "Title: Marouane Temimi | Stevens Institute of Technology\n",
            "Keywords found: machine learning, remote sensing, hydrology\n",
            "Emails found: mtemimi@stevens.edu\n",
            "---------------------------------------------\n",
            "URL: https://www.stevens.edu/profile/dvaccari\n",
            "Title: David Vaccari | Stevens Institute of Technology\n",
            "Keywords found: water quality, remote sensing\n",
            "Emails found: dvaccari@stevens.edu\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Function to extract email addresses from text using regex\n",
        "def extract_emails(text):\n",
        "    emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "    return emails\n",
        "\n",
        "# Function to extract valid absolute URLs from a primary URL\n",
        "def extract_urls(primary_url):\n",
        "    try:\n",
        "        # Send a GET request to the primary URL\n",
        "        response = requests.get(primary_url)\n",
        "\n",
        "        # Check if request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find all 'a' tags with 'href' attribute\n",
        "            links = soup.find_all('a', href=True)\n",
        "\n",
        "            # Extract and filter URLs from href attributes\n",
        "            urls = []\n",
        "            for link in links:\n",
        "                url = link.get('href')\n",
        "                if url:\n",
        "                    # If it's a relative URL, convert it to absolute URL\n",
        "                    absolute_url = urljoin(primary_url, url)\n",
        "                    if absolute_url.startswith('http://') or absolute_url.startswith('https://'):\n",
        "                        urls.append(absolute_url)\n",
        "                    else:\n",
        "                        # If it's neither http:// nor https://, skip it\n",
        "                        continue\n",
        "\n",
        "            # # Filter out non-absolute URLs and keep only those starting with 'https://www.stonybrook.edu/commcms/civileng/people/_core_faculty/'\n",
        "            # urls = [url for url in urls if url.startswith('https://www.stonybrook.edu/commcms/civileng/people/_core_faculty/')]\n",
        "\n",
        "            return urls\n",
        "        else:\n",
        "            # Request was not successful, handle error\n",
        "            print(f\"Error fetching {primary_url}: Status Code {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {primary_url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {primary_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Primary URL to extract URLs from\n",
        "primary_url = 'https://ecs.syracuse.edu/faculty-staff/?category=civil-and-environmental-engineering&people='\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = ['water quality', 'machine learning', 'remote sensing', 'hydrology']\n",
        "\n",
        "# Function to scrape URLs, search for keywords, and extract emails\n",
        "def scrape_urls(urls, keywords):\n",
        "    visited_urls = set()  # Set to keep track of visited URLs\n",
        "    for url in urls:\n",
        "        if url not in visited_urls:\n",
        "            visited_urls.add(url)  # Mark URL as visited\n",
        "\n",
        "            try:\n",
        "                # Fetch the webpage content\n",
        "                response = requests.get(url)\n",
        "                if response.status_code == 200:\n",
        "                    html = response.text\n",
        "                    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                    # Extract title of the page\n",
        "                    title = soup.title.text.strip() if soup.title else \"No title\"\n",
        "\n",
        "                    # Check for keywords in the page content\n",
        "                    found_keywords = []\n",
        "                    for keyword in keywords:\n",
        "                        if soup.body and soup.body.find_all(string=re.compile(r'\\b{}\\b'.format(re.escape(keyword))), recursive=True):\n",
        "                            found_keywords.append(keyword)\n",
        "\n",
        "                    # Extract email addresses from the page content\n",
        "                    emails_in_page = extract_emails(html)\n",
        "\n",
        "                    # If any keywords are found, print the results\n",
        "                    if found_keywords:\n",
        "                        print(f\"URL: {url}\")\n",
        "                        print(f\"Title: {title}\")\n",
        "                        print(f\"Keywords found: {', '.join(found_keywords)}\")\n",
        "                        print(f\"Emails found: {', '.join(set(emails_in_page))}\")  # Use set() to remove duplicates\n",
        "                        print(\"---------------------------------------------\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching URL: {url}\")\n",
        "                print(e)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL: {url}\")\n",
        "                print(e)\n",
        "\n",
        "# Extract URLs from primary URL\n",
        "urls = extract_urls(primary_url)\n",
        "\n",
        "# Call the function to scrape URLs, search for keywords, and extract emails\n",
        "scrape_urls(urls, keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw6rUPot9xzE",
        "outputId": "dc5216d2-aff5-4ab7-dab0-c58d18c583c8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://ecs.syracuse.edu/student-services/clubs-and-organizations\n",
            "Title: Clubs and Organizations - ECS â€“ Syracuse University\n",
            "Keywords found: remote sensing\n",
            "Emails found: \n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/shobha-k-bhatia\n",
            "Title: Shobha K. Bhatia - ECS â€“ Syracuse University\n",
            "Keywords found: water quality\n",
            "Emails found: skbhatia@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/elizabeth-carter\n",
            "Title: Elizabeth Carter - ECS â€“ Syracuse University\n",
            "Keywords found: machine learning\n",
            "Emails found: ekcarter@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/david-chandler\n",
            "Title: David Chandler - ECS â€“ Syracuse University\n",
            "Keywords found: hydrology\n",
            "Emails found: dgchandl@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/charles-t-driscoll\n",
            "Title: Charles T. Driscoll - ECS â€“ Syracuse University\n",
            "Keywords found: machine learning\n",
            "Emails found: ctdrisco@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/min-liu\n",
            "Title: Min Liu - ECS â€“ Syracuse University\n",
            "Keywords found: machine learning\n",
            "Emails found: mliu92@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/yizhi-liu\n",
            "Title: Yizhi Liu - ECS â€“ Syracuse University\n",
            "Keywords found: machine learning\n",
            "Emails found: yliu580@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/aaron-mohammed\n",
            "Title: Aaron Mohammed - ECS â€“ Syracuse University\n",
            "Keywords found: hydrology\n",
            "Emails found: aamohamm@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/svetoslava-todorova\n",
            "Title: Svetoslava Todorova - ECS â€“ Syracuse University\n",
            "Keywords found: water quality\n",
            "Emails found: stodorov@syr.edu\n",
            "---------------------------------------------\n",
            "URL: https://ecs.syracuse.edu/faculty-staff/john-trimmer\n",
            "Title: John Trimmer - ECS â€“ Syracuse University\n",
            "Keywords found: machine learning\n",
            "Emails found: jttrimme@syr.edu\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-ASCkTKBYgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}